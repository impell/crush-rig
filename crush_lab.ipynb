{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crush Rig - LAB\n",
    "Written by Matt MacDonald for CIGITI at the Hospital for Sick Children Toronto\n",
    "\n",
    "### This notebook is to explore alternative models than baseline logistic regression.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All tools to manipulate data will be obtained from the crush_plot.py file. The objective of this notebook is to predict the histological targets from the force/position crush data using a classifier, either logistic regression or otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdb import set_trace\n",
    "from warnings import warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crush_read import *\n",
    "from crush_plot import *\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The crush data must be collected using the crush rig and crush.py and stored in the expected folder structure at the root directory indicated by PATH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH = Path('')\n",
    "# Default in crush_plot.py\n",
    "PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all data and modify as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = study_outline(PATH)\n",
    "targets = study_targets(PATH)\n",
    "crushes = study_data(study)\n",
    "crushes = modify(crushes)\n",
    "crushes = calculate(crushes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, legend = prep(crushes, targets)\n",
    "y = refine(y)\n",
    "print('Reference for categorical features:')\n",
    "legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in y.columns:\n",
    "    most_common = y[col].value_counts().idxmax()\n",
    "    s = (y[col] == most_common).sum()\n",
    "    c = y[col].count()\n",
    "    r = s / c\n",
    "    print(f\"{col}\\nBaseline Accuracy = {s}/{c} ({r:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove any histology related features to focus on real time predictors. Also remove the holding strain since only the STOP protocol is being considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full = X.copy()\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop('Pathologist (Cathy or Corwyn)', axis=1)\n",
    "X = X.drop('Serosal Thickness (mm)', axis=1)\n",
    "X = X.drop('Post Serosal Thickness (mm)', axis=1)\n",
    "X = X.drop('Holding Strain', axis=1)\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal for the prediction algorithm is to provide a metric for preventing tissue damage intraoperatively. Thus it has the following requirements:\n",
    "\n",
    "1. Good overall accuracy so it is reliable without being restrictive\n",
    "2. High recall such that it is conservative, limiting the occurrence of false negatives\n",
    "3. Simple with limited input so that it can be implemented cheaply in real time\n",
    "\n",
    "Further to requirement 3 above, no histology features can be used to make the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show correlations for the reduced feature set\n",
    "X_corr = X.corr(method='spearman')\n",
    "sns.heatmap(X_corr, cmap='RdBu', vmin=-1, vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Mix Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Major Tissue Damage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a specific indicator from the targets and split the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicators = ['Significant Serosal Change',\n",
    "              'Tissue Damage',\n",
    "              'Major Tissue Damage']\n",
    "indicator_labels = {'Significant Serosal Change': ['No Change', 'Significant Change'],\n",
    "                    'Tissue Damage': ['No Damage', 'Damage'],\n",
    "                    'Major Tissue Damage': ['No Damage or Minor Damage', 'Major Damage']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = indicators[2]\n",
    "ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only 3 positive examples for major damage!!\n",
    "y[ind].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, 3 positive samples for major damage is not enough to form a useful model. So instead an anomaly detection algorithm will be applied to look for deviations from the normal expectation. For simplicity the validation will be excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anom_split(X, y, seed=0):\n",
    "    X_np = X.values.astype(np.float64)\n",
    "    y_np = y.values\n",
    "\n",
    "    pos = (y_np == 1)\n",
    "    y_pos = y_np[pos].reshape([-1, 1])\n",
    "    X_pos = X_np[pos]\n",
    "    y_neg = y_np[~pos].reshape([-1, 1])\n",
    "    X_neg = X_np[~pos]\n",
    "\n",
    "    size = 0.2\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_neg, y_neg, test_size=size, random_state=seed)\n",
    "    X_test = np.vstack([X_test, X_pos])\n",
    "    y_test = np.vstack([y_test, y_pos])\n",
    "    \n",
    "    return (X_train, y_train), (X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anom_prob_dist_GMM(dataset, n_comp=1, seed=0):\n",
    "    model = GaussianMixture(n_components=n_comp,\n",
    "                            covariance_type='full',\n",
    "                            random_state=seed)\n",
    "    model.fit(dataset[0])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anom_stats(X):\n",
    "    mu = np.mean(X, axis=0)\n",
    "    Sigma2 = np.diag(np.var(X, axis=0))\n",
    "    return mu, Sigma2\n",
    "\n",
    "def anom_prob_dist(X, mu, Sigma2):\n",
    "    k = mu.size\n",
    "    X = X - mu\n",
    "    p = (2 * np.pi) ** (- k / 2) * np.linalg.det(Sigma2) ** (-0.5)\\\n",
    "        * np.exp(-0.5 * np.sum(np.dot(X, np.linalg.pinv(Sigma2)) * X, axis=1))\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anom_best_eps(pval, yval, disp=False):\n",
    "    eps = np.linspace(1.01 * min(pval), 0.99 * max(pval), 1000)\n",
    "    F1 = np.zeros(eps.shape)\n",
    "    for i, epsilon in enumerate(eps):\n",
    "        yhat = (pval <= epsilon)\n",
    "        tp = ((yval == 1) & yhat).sum()\n",
    "        fp = ((yval == 0) & yhat).sum()\n",
    "        fn = ((yval == 1) & ~yhat).sum()\n",
    "        \n",
    "        if tp > 0:\n",
    "            prec = tp / (tp + fp)\n",
    "            rec = tp / (tp + fn)\n",
    "            F1[i] = 2 * prec * rec / (prec + rec)\n",
    "    \n",
    "    if disp:\n",
    "        plt.plot(eps, F1)\n",
    "    idx = np.argmax(F1)\n",
    "    return eps[idx], F1[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train, ds_test = anom_split(X, y[ind])\n",
    "mu, S2 = anom_stats(ds_train[0])\n",
    "probs = anom_prob_dist(ds_test[0], mu, S2)\n",
    "y_test = ds_test[1].ravel()\n",
    "eps = probs[y_test == 1].max()\n",
    "eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestEps, F1 = anom_best_eps(probs, y_test, disp=True)\n",
    "print(f\"epsilon = {bestEps:.6f}, F1 = {F1:.3f}\")\n",
    "print('Pretty bad..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a 2D version of the data, highlighting damage and major damage\n",
    "X_np = X.values.astype(np.float64)\n",
    "scaler = StandardScaler()\n",
    "X_std = scaler.fit_transform(X_np)\n",
    "\n",
    "masks = [y['Tissue Damage'] == 0,\n",
    "         y['Tissue Damage'] == 1,\n",
    "         y['Major Tissue Damage'] == 1]\n",
    "masks[1][masks[2]] = False  # exclude damage score 2 from set of score 1\n",
    "\n",
    "# Find outliers using simple gaussian\n",
    "mu, S2 = anom_stats(X_std)\n",
    "probs = anom_prob_dist(X_std, mu, S2)\n",
    "eps = anom_best_eps(probs, masks[2])[0]\n",
    "outliers = probs <= eps\n",
    "\n",
    "X_0 = X_std[masks[0], :]\n",
    "X_1 = X_std[masks[1], :]\n",
    "X_2 = X_std[masks[2], :]\n",
    "\n",
    "cov = X_std.T @ X_std / X_std.shape[0]\n",
    "U, S, V = np.linalg.svd(cov)\n",
    "\n",
    "X_2D_0 = X_0 @ U[:, :2]\n",
    "X_2D_1 = X_1 @ U[:, :2]\n",
    "X_2D_2 = X_2 @ U[:, :2]\n",
    "\n",
    "for i, X_2D in enumerate([X_2D_0, X_2D_1, X_2D_2]):\n",
    "    plt.scatter(X_2D[:, 0], X_2D[:, 1], label=f'Trauma score {i}')\n",
    "    plt.plot(X_2D[outliers[masks[i]], 0],\n",
    "             X_2D[outliers[masks[i]], 1],\n",
    "             'ro', ms=10, mfc='None', mew=2)\n",
    "plt.legend(loc='lower right')\n",
    "print('Retained {:.2%}% of the variance'.format(S[:2].sum() / S.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scales = scaler.scale_[:, None]\n",
    "eig = U[:, :2] / np.concatenate([scales, scales], axis=1)\n",
    "for i, name in enumerate(X.columns):\n",
    "    print(f\"{name:30s}{eig[i, 0]:6.2f}, {eig[i, 1]:6.2f}\\tScale = {eig[i, 0] / eig[i, 1]:+.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max in both dimensions\n",
    "idx = X_2D_1.argmax(axis=0)\n",
    "for i, name in enumerate(X.columns):\n",
    "    val_1 = X_1[idx[0], i]\n",
    "    val_2 = X_1[idx[1], i]\n",
    "    print(f\"{name:30s}Max X1 = {val_1:+10.2f}\\tMax X2 = {val_2:+10.2f}\\tSpread = {abs(val_1 - val_2):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It isn't super clear how to divide the segments but there are clear differences. The division between trauma score 1 and 2 is there so it should be possible in theory with an GMM to define a boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a GMM on the data\n",
    "dataset_train, dataset_test = anom_split(X, y[ind], seed=SEED)\n",
    "model = anom_prob_dist(dataset_train, 2, seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(model.score_samples(dataset_train[0])).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = np.exp(model.score_samples(dataset_test[0])).round(5)\n",
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = dataset_test[1].reshape([-1]).astype(np.bool)\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = np.max(prob[y_test == 1])\n",
    "eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob <= eps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not a very effective model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def log_fn(x):\n",
    "    if x.dtype == 'bool':\n",
    "        x = x.astype('float64')\n",
    "    if np.any(x == 0):\n",
    "        x = x + 0.001\n",
    "    return np.log(x)\n",
    "\n",
    "X_log = X.apply(log_fn)\n",
    "for col in X.columns:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    X[col].hist(ax=ax1)\n",
    "    X_log[col].hist(ax=ax2)\n",
    "    fig.suptitle(f'{col} - Normal and Log')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certain features, namely thickness, crush duration and relaxation stress, are more normally distributed when using the log of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "clf = XGBClassifier()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "y_corr = y_pred == y_test"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "y_pred_train = clf.predict(X_train)\n",
    "y_corr_train = y_pred_train == y_train"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(f\"test acc = {sum(y_corr) / len(y_corr)}\")\n",
    "print(f\"train acc = {sum(y_corr_train) / len(y_corr_train)}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from xgboost import plot_tree\n",
    "plot_tree(clf, rankdir='LR', num_trees=3)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# feature numbers legend\n",
    "for i, feat in enumerate(X.columns):\n",
    "    print(f\"f{i} = {feat}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X.corr()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df = pd.concat([X, y], axis=1)\n",
    "df.corr()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for prot in [0, 1]:\n",
    "    prot_str = f\"Protocol[{prot}]\"\n",
    "    avg = df.loc[df['Protocol'] == prot, 'Damage Score'].mean()\n",
    "    print(f\"{legend[prot_str]} average damage = {avg}\")\n",
    "          "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
