{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crush Rig - LAB\n",
    "Written by Matt MacDonald for CIGITI at the Hospital for Sick Children Toronto\n",
    "\n",
    "### This notebook is to explore alternative models than baseline logistic regression.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All tools to manipulate data will be obtained from the crush_plot.py file. The objective of this notebook is to predict the histological targets from the force/position crush data using a classifier, either logistic regression or otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdb import set_trace\n",
    "from warnings import warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crush_read import *\n",
    "from crush_plot import *\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The crush data must be collected using the crush rig and crush.py and stored in the expected folder structure at the root directory indicated by PATH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH = Path('')\n",
    "# Default in crush_plot.py\n",
    "PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all data and modify as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = study_outline(PATH)\n",
    "targets = study_targets(PATH)\n",
    "crushes = study_data(study)\n",
    "crushes = modify(crushes)\n",
    "crushes = calculate(crushes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, legend = preprocess(crushes, targets)\n",
    "y = binary_classes(y)\n",
    "print('Reference for categorical features:')\n",
    "legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in y.columns:\n",
    "    most_common = y[col].value_counts().idxmax()\n",
    "    s = (y[col] == most_common).sum()\n",
    "    c = y[col].count()\n",
    "    r = s / c\n",
    "    print(f\"{col}\\nBaseline Accuracy = {s}/{c} ({r:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove any histology related features to focus on real time predictors. Also remove the holding strain since only the STOP protocol is being considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full = X.copy()\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop('Holding Strain', axis=1)\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal for the prediction algorithm is to provide a metric for preventing tissue damage intraoperatively. Thus it has the following requirements:\n",
    "\n",
    "1. Good overall accuracy so it is reliable without being restrictive\n",
    "2. High recall such that it is conservative, limiting the occurrence of false negatives\n",
    "3. Simple with limited input so that it can be implemented cheaply in real time\n",
    "\n",
    "Further to requirement 3 above, no histology features can be used to make the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Show correlations for the reduced feature set\n",
    "X_corr = X.corr(method='spearman')\n",
    "sns.heatmap(X_corr, cmap='RdBu', vmin=-1, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_full = y.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_fn(x):\n",
    "    if x.dtype == 'bool':\n",
    "        x = x.astype('float64')\n",
    "    if np.any(x == 0):\n",
    "        x = x + 0.001\n",
    "    return np.log(x)\n",
    "\n",
    "X_log = X.apply(log_fn)\n",
    "for col in X.columns:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    X[col].hist(ax=ax1)\n",
    "    X_log[col].hist(ax=ax2)\n",
    "    fig.suptitle(f'{col} - Normal and Log');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certain features, namely thickness, crush duration and relaxation stress, are more normally distributed when using the log of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Builder\n",
    "Function to iteratively build and fit a number of common binary classifiers in sklearn. Inspired and forked from https://github.com/VinGPan/Machine_Learning_3252_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models, transforms and model selection tools to explore\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "# Sklearn utilities\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models will attempt to predict significant serosa change or tissue damage. Both are histology based metrics of tissue trauma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y[['Significant Serosa Change', 'Tissue Damage']]\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions for modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifiers(all_scores):  \n",
    "    # Find top three models\n",
    "    all_scores = sorted(all_scores, key=lambda x: x[3], reverse=True)  # sort by accuracy\n",
    "    prev_cls = None\n",
    "    cls_count = 0\n",
    "    top_scores = []\n",
    "    for score in all_scores:\n",
    "        if prev_cls == score[0]:\n",
    "            continue\n",
    "\n",
    "        prev_cls = score[0]\n",
    "        cls_count += 1\n",
    "        top_scores.append(score)\n",
    "        \n",
    "        if cls_count == 3:\n",
    "            break\n",
    "    \n",
    "    col_names = ['classifier', 'scaler', 'transform', 'accuracy',\n",
    "                 'balanced_accuracy', 'f1_score', 'auc_score', 'params', 'model']\n",
    "    top_scores = pd.DataFrame(np.array(top_scores), columns=col_names)\n",
    "    all_scores = pd.DataFrame(np.array(all_scores), columns=col_names)\n",
    "    \n",
    "    return all_scores, top_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifiers(target_idx, exp_name):\n",
    "    '''\n",
    "    This function provides logic for building several binary classification models and performing\n",
    "    data transformations, dimensionality reduction, and tuning using GridSearchCV.\n",
    "\n",
    "    This function stores all the intermediate results so that one can stop and restart.\n",
    "\n",
    "    NOTE: If you have already run this experiment, this function will simply load the old results.\n",
    "\n",
    "    :target_idx: which target to use for training\n",
    "    :exp_name: name of the experiment for saving, suggest 'expN'\n",
    "    :return: all model scores, top three model scores\n",
    "    '''\n",
    "    \n",
    "    config = {'classifiers': ['logistic',\n",
    "                              'naive_bayes',\n",
    "                              'knn', 'random_forest',\n",
    "                              'xgboost',\n",
    "                              'adaboost',\n",
    "                              'gradboost',\n",
    "                              'svc'],\n",
    "              'scalers': ['none', 'min_max', 'standard_scalar'],\n",
    "              'transforms': ['none', 'pca']}\n",
    "\n",
    "    # Split data into training set\n",
    "    ratio = 0.8\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X.values, y.values[:, target_idx], train_size=ratio)\n",
    "\n",
    "    # Create output directory\n",
    "    try:\n",
    "        os.mkdir(\"output/\" + exp_name + \"/\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Check if models already fit\n",
    "    if os.path.exists(\"output/\" + exp_name + \"/all_scores.pkl\"):\n",
    "        all_scores = pickle.load(open(\"output/\" + exp_name + \"/all_scores.pkl\", \"rb\"))\n",
    "        \n",
    "        return evaluate_classifiers(all_scores)\n",
    "        \n",
    "    # Otherwise fit all models\n",
    "    all_scores = []\n",
    "    count = 0\n",
    "    for clf_str in config['classifiers']:\n",
    "        for scl_str in config['scalers']:\n",
    "            for tfm_str in config['transforms']:\n",
    "                count += 1\n",
    "                steps = [('imputer', SimpleImputer(strategy='mean'))]\n",
    "                param_grid = [{}]\n",
    "\n",
    "                # Pick a data scaler\n",
    "                if scl_str == 'min_max':\n",
    "                    steps.append(('scaler', MinMaxScaler()))\n",
    "                elif scl_str == 'standard_scalar':\n",
    "                    steps.append(('scaler', StandardScaler()))\n",
    "\n",
    "                # Pick a dimensionality reduction transformation\n",
    "                if tfm_str == 'pca':\n",
    "                    steps.append(('transform', PCA()))\n",
    "                    param_grid[0][\"transform__n_components\"] = [None, 0.95, 0.9]\n",
    "\n",
    "                # Pick a binary classifier\n",
    "                if clf_str == 'logistic':\n",
    "                    steps.append(('clf', LogisticRegression(multi_class='auto', random_state=0, solver='liblinear')))\n",
    "                    param_grid[0][\"clf__penalty\"] = ['l1', 'l2']\n",
    "                    param_grid[0][\"clf__C\"] = [0.01, 0.1, 1, 10]\n",
    "                    param_grid[0][\"clf__class_weight\"] = [None, 'balanced']\n",
    "                elif clf_str == 'naive_bayes':\n",
    "                    steps.append(('clf', GaussianNB()))\n",
    "                elif clf_str == 'knn':\n",
    "                    steps.append(('clf', KNeighborsClassifier()))\n",
    "                    param_grid[0][\"clf__n_neighbors\"] = [3, 5, 10, 20]\n",
    "                    param_grid[0][\"clf__weights\"] = ['uniform', 'distance']\n",
    "                    param_grid[0][\"clf__metric\"] = ['euclidean', 'manhattan']\n",
    "                elif clf_str == 'random_forest':\n",
    "                    steps.append(('clf', RandomForestClassifier()))\n",
    "                    param_grid[0][\"clf__max_depth\"] = [3, 7, 10, 20]\n",
    "                    param_grid[0][\"clf__min_samples_split\"] = [10, 15, 30]\n",
    "                    param_grid[0][\"clf__n_estimators\"] = [50, 100, 150, 200]\n",
    "                elif clf_str == 'svc':\n",
    "                    steps.append(('clf', SVC(class_weight='balanced', random_state=42)))\n",
    "                    param_grid[0][\"clf__kernel\"] = ['linear', 'rbf']\n",
    "                    param_grid[0][\"clf__C\"] = [0.01, 0.1, 1]\n",
    "                elif clf_str == 'xgboost':\n",
    "                    steps.append(('clf', xgb.XGBClassifier(random_state=42, objective='binary:logistic')))\n",
    "                    param_grid[0][\"clf__learning_rate\"] = [0.001, 0.01, 0.1]\n",
    "                    param_grid[0][\"clf__n_estimators\"] = [50, 100, 150, 200]\n",
    "                elif clf_str == 'adaboost':\n",
    "                    steps.append(('clf', AdaBoostClassifier(random_state=42)))\n",
    "                    param_grid[0][\"clf__n_estimators\"] = [50, 100, 150, 200]\n",
    "                elif clf_str == 'gradboost':\n",
    "                    steps.append(('clf', GradientBoostingClassifier(random_state=42)))\n",
    "                    param_grid[0][\"clf__learning_rate\"] = [0.001, 0.01, 0.1]\n",
    "                    param_grid[0][\"clf__n_estimators\"] = [50, 100, 150, 200]\n",
    "\n",
    "                # Perform grid search\n",
    "                pipeline = Pipeline(steps=steps)\n",
    "                clf = GridSearchCV(estimator=pipeline, cv=3, refit=True,\n",
    "                                   param_grid=param_grid, verbose=1, scoring='balanced_accuracy')\n",
    "                \n",
    "                res_path = \"output/\" + exp_name + \"/\" + clf_str + \"_\" + scl_str + \"_\" + tfm_str + \".pkl\"\n",
    "                if os.path.exists(res_path):\n",
    "                    clf = joblib.load(res_path)\n",
    "                else:\n",
    "                    try:\n",
    "                        clf.fit(X_train, y_train)\n",
    "                    except:\n",
    "                        print(\"Crash for \" + res_path)\n",
    "                        continue\n",
    "                    \n",
    "                    # Store the model\n",
    "                    joblib.dump(clf, res_path)\n",
    "                    \n",
    "                y_hat = clf.predict(X_test)\n",
    "\n",
    "                # Compute accuracy scores\n",
    "                acc = accuracy_score(y_test, y_hat)\n",
    "                bal_acc = balanced_accuracy_score(y_test, y_hat)\n",
    "                f1 = f1_score(y_test, y_hat)\n",
    "                auc = roc_auc_score(y_test, y_hat)\n",
    "\n",
    "                all_scores.append([clf_str, scl_str, tfm_str, acc, bal_acc, f1, auc, clf.best_params_, clf])\n",
    "\n",
    "        pickle.dump(all_scores, open(\"output/\" + exp_name + \"/all_scores.pkl\", \"wb\"))\n",
    "        \n",
    "    print(f'Fit and tuned {count} total models!')\n",
    "    return evaluate_classifiers(all_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full = X.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target 1 - Serosa Thickness\n",
    "Predict significant serosa change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_idx = 0\n",
    "y.columns[target_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores, top_scores = build_classifiers(target_idx, 'serosa_sig2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores[all_scores['classifier'] == 'logistic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_1_scores = [all_scores, top_scores]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target 2 - Trauma Score\n",
    "Predict trauma score of 1 or greater."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_idx = 1\n",
    "y.columns[target_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores, top_scores = build_classifiers(target_idx, 'tissue_dam2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores[all_scores['classifier'] == 'logistic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_2_scores = [all_scores, top_scores]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stress only as a metric\n",
    "### Trauma Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_idx = 1\n",
    "y.columns[target_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.loc[:, ['Target Stress (MPa)']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores, top_scores = build_classifiers(target_idx, 'tissue_dam_stress_only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores[all_scores['classifier'] == 'logistic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_2_stress_only_scores = [all_scores, top_scores]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serosa Thickness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_idx = 0\n",
    "y.columns[target_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores, top_scores = build_classifiers(target_idx, 'serosa_sig_stress_only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores[all_scores['classifier'] == 'logistic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_1_stress_only_scores = [all_scores, top_scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
